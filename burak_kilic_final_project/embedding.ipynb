{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11527475,"sourceType":"datasetVersion","datasetId":7229884}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Gerekli kütüphaneleri yükle\n!pip install -qU chromadb sentence-transformers langchain langchain_community langchain-text-splitters accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T10:57:07.126000Z","iopub.execute_input":"2025-04-23T10:57:07.126204Z","iopub.status.idle":"2025-04-23T10:59:02.153791Z","shell.execute_reply.started":"2025-04-23T10:57:07.126189Z","shell.execute_reply":"2025-04-23T10:59:02.152768Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport torch # GPU kontrolü için\n\nprint(\"Kütüphaneler yüklendi.\")\n\n# --- 1. Ayarlar ---\n# Veri dosyalarının bulunduğu Kaggle yolu (kendi yüklediğiniz klasör adıyla değiştirin)\n# Örnek: Eğer dosyaları \"resmi-gazete-txt\" adlı bir datasete yüklediyseniz:\n# data_folder = \"/kaggle/input/resmi-gazete-txt\"\n# Eğer direkt upload ettiyseniz ve Kaggle bir isim verdiyse o ismi kullanın.\n# Sağdaki Input panelinden klasör ismini kontrol edebilirsiniz.\nDATA_FOLDER = \"/kaggle/input/gazete-txt\" # <-- BURAYI GÜNCELLEYİN!\n\n# ChromaDB'nin kaydedileceği yer (Kaggle çalışma dizini)\nCHROMA_PERSIST_DIR = \"/kaggle/working/chroma_db_bge_m3\"\n# ChromaDB Koleksiyon adı\nCHROMA_COLLECTION_NAME = \"resmi_gazete_bge_m3\"\n\n# Embedding Modeli\nEMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n\n# Metin Bölme Ayarları\nCHUNK_SIZE = 1000 # Her bir parçanın karakter sayısı (deneyerek optimize edilebilir)\nCHUNK_OVERLAP = 150 # Parçalar arası karakter örtüşmesi\n\n# Cihazı Ayarla (GPU varsa CUDA kullan, yoksa CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Kullanılan Cihaz: {device}\")\nif not torch.cuda.is_available():\n    print(\"UYARI: GPU bulunamadı, embedding işlemi CPU üzerinde yavaş olabilir.\")\n\n# --- 2. Veri Yükleme ve Hazırlama ---\nprint(f\"\\nVeri klasörü kontrol ediliyor: {DATA_FOLDER}\")\nif not os.path.exists(DATA_FOLDER):\n    raise FileNotFoundError(f\"HATA: Belirtilen veri klasörü bulunamadı: {DATA_FOLDER}. \"\n                          \"Lütfen DATA_FOLDER değişkenini doğru ayarladığınızdan emin olun.\")\n\nall_docs = []\ndoc_count = 0\ntxt_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(\".txt\")]\n\nif not txt_files:\n     raise FileNotFoundError(f\"HATA: {DATA_FOLDER} içinde '.txt' uzantılı dosya bulunamadı.\")\n\nprint(f\"{len(txt_files)} adet .txt dosyası bulundu. Okunuyor...\")\n\nfor filename in txt_files:\n    file_path = os.path.join(DATA_FOLDER, filename)\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            # Langchain Document objesi oluşturuyoruz (metadata ile)\n            # Langchain'in kendi Document yapısını kullanmak yerine string listesi de olurdu\n            # ama metadata eklemek için bu yapı kullanışlı.\n            # Şimdilik basit metadata ekleyelim:\n            all_docs.append({\"page_content\": content, \"metadata\": {\"source\": filename}})\n            doc_count += 1\n            print(f\" - Okundu: {filename}\")\n    except Exception as e:\n        print(f\"HATA: {filename} okunurken hata oluştu: {e}\")\n\nprint(f\"\\nToplam {doc_count} doküman başarıyla okundu.\")\n\n# --- 3. Metin Bölme (Chunking) ---\nprint(\"\\nMetinler parçalara (chunk) ayrılıyor...\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n    length_function=len,\n    add_start_index=True, # Her parçanın başlangıç indeksini metadata'ya ekler\n)\n\nchunks = []\nfor doc in all_docs:\n    # text_splitter.create_documents direkt Document objeleriyle çalışır\n    # Bizim yapımız biraz farklı olduğu için split_text kullanalım\n    doc_content = doc[\"page_content\"]\n    doc_metadata = doc[\"metadata\"]\n    \n    split_texts = text_splitter.split_text(doc_content)\n    \n    for i, text_chunk in enumerate(split_texts):\n        # Her chunk için metadata'yı genişletelim\n        chunk_metadata = doc_metadata.copy()\n        chunk_metadata[\"chunk_index\"] = i \n        # İleride chunk'ın hangi dokümanın hangi parçası olduğunu bilmek önemli\n        \n        chunks.append({\"page_content\": text_chunk, \"metadata\": chunk_metadata})\n\nprint(f\"Toplam {len(chunks)} adet metin parçası (chunk) oluşturuldu.\")\n\n# Bellek optimizasyonu için orijinal dokümanları silebiliriz\ndel all_docs\n\n# --- 4. Embedding Modelini Yükleme ---\nprint(f\"\\nEmbedding modeli yükleniyor: {EMBEDDING_MODEL_NAME}...\")\n# Modeli belirtilen cihaza (GPU/CPU) yükle\nembedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\nprint(\"Embedding modeli yüklendi.\")\n\n# --- 5. Embedding Oluşturma ---\nprint(\"\\nMetin parçaları için embeddingler oluşturuluyor (Bu işlem zaman alabilir)...\")\n\n# Sadece metin içeriklerini alıp listeye çevir\ntexts_to_embed = [chunk[\"page_content\"] for chunk in chunks]\n\nstart_time = time.time()\n# Modeli kullanarak embeddingleri hesapla\n# show_progress_bar=True ile ilerlemeyi gör\nembeddings = embedding_model.encode(\n    texts_to_embed,\n    show_progress_bar=True,\n    batch_size=32 # GPU belleğine göre ayarlanabilir\n)\nend_time = time.time()\n\nprint(f\"Embedding oluşturma tamamlandı. Süre: {end_time - start_time:.2f} saniye.\")\nprint(f\"Oluşturulan embedding matris boyutu: {embeddings.shape}\") # (chunk_sayisi, embedding_boyutu)\n\n# Bellek Optimizasyonu\ndel texts_to_embed\n\n# --- 6. ChromaDB Kurulumu ve Veri Yükleme ---\nprint(f\"\\nChromaDB kuruluyor ve veriler yükleniyor...\")\nprint(f\"Veritabanı yolu: {CHROMA_PERSIST_DIR}\")\n\n# PersistentClient ile veritabanını diske kaydet\n# Eğer klasör varsa içeriğiyle birlikte yükler, yoksa oluşturur.\nchroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n\n# Koleksiyonu al veya oluştur (Embedding fonksiyonu belirtmiyoruz, çünkü embeddingleri biz sağladık)\n# ÖNEMLİ: Eğer daha önce aynı isimde farklı bir embedding modeliyle koleksiyon oluşturduysanız\n# sorun yaşamamak için ya eski klasörü silin ya da farklı bir koleksiyon adı kullanın.\ncollection = chroma_client.get_or_create_collection(\n    name=CHROMA_COLLECTION_NAME,\n    # metadata={\"hnsw:space\": \"cosine\"} # BGE modelleri genellikle cosine similarity ile iyi çalışır, Chroma varsayılanı L2'dir. İsteğe bağlı.\n)\nprint(f\"'{CHROMA_COLLECTION_NAME}' koleksiyonu alındı veya oluşturuldu.\")\n\n# ChromaDB'ye eklemek için ID'ler ve metadatalar oluşturalım\nids = [f\"{chunk['metadata']['source']}_chunk_{chunk['metadata']['chunk_index']}\" for chunk in chunks]\nmetadatas = [chunk[\"metadata\"] for chunk in chunks]\ndocuments = [chunk[\"page_content\"] for chunk in chunks] # ChromaDB dökümanları da saklar\n\n# Veriyi ChromaDB'ye ekle\n# Eğer çok fazla chunk varsa (örn. > 50k), batch'ler halinde eklemek daha iyi olabilir.\n# Şu anki miktar için tek seferde eklemek sorun olmamalı.\nprint(f\"{len(ids)} adet chunk ChromaDB'ye ekleniyor...\")\ntry:\n    collection.add(\n        embeddings=embeddings.tolist(), # NumPy array'i listeye çevir\n        documents=documents,\n        metadatas=metadatas,\n        ids=ids\n    )\n    print(\"Veriler başarıyla ChromaDB'ye eklendi.\")\nexcept Exception as e:\n    print(f\"HATA: ChromaDB'ye veri eklenirken sorun oluştu: {e}\")\n    # Olası sorun: Aynı ID ile tekrar ekleme yapmaya çalışmak.\n    # Eğer kodu tekrar çalıştırıyorsanız ve hata alıyorsanız, önce /kaggle/working/chroma_db_bge_m3 klasörünü silmeyi deneyin.\n\n# Eklenen öğe sayısını kontrol et\ncount = collection.count()\nprint(f\"Koleksiyondaki toplam öğe sayısı: {count}\")\n\n# Bellek Optimizasyonu\ndel embeddings, documents, metadatas, ids, chunks\n\n# --- 7. Sonuç ve İndirme ---\nprint(f\"\\nİşlem tamamlandı!\")\nprint(f\"ChromaDB vektör deposu '{CHROMA_PERSIST_DIR}' klasörüne kaydedildi.\")\nprint(\"\\nSonraki Adımlar:\")\nprint(\"1. Bu Kaggle Notebook'unu 'Save Version' ile kaydedin (Type: 'Run All', Save Output: 'Yes').\")\nprint(\"2. Kaydetme işlemi bittikten sonra, Notebook versiyon sayfasının 'Output' sekmesine gidin.\")\nprint(f\"3. Orada '{os.path.basename(CHROMA_PERSIST_DIR)}' adlı klasörü bulun ve indirin.\")\nprint(\"4. İndirdiğiniz bu klasörü lokal makinenizde RAG projenizde kullanabilirsiniz.\")\nprint(\"\\nLokalde Kullanım Örneği:\")\nprint(\"```python\")\nprint(\"import chromadb\")\nprint(\"# İndirdiğiniz klasörün yolunu verin\")\nprint(f\"LOCAL_CHROMA_PATH = '/path/to/your/downloaded/{os.path.basename(CHROMA_PERSIST_DIR)}'\")\nprint(f\"PERSISTENT_CLIENT = chromadb.PersistentClient(path=LOCAL_CHROMA_PATH)\")\nprint(f\"COLLECTION = PERSISTENT_CLIENT.get_collection(name='{CHROMA_COLLECTION_NAME}')\")\nprint(\"# Sorgu yapmak için yine BAAI/bge-m3 modelini kullanmalısınız!\")\nprint(\"# query_embedding = embedding_model.encode(['Sorgunuz buraya'])\")\nprint(\"# results = COLLECTION.query(query_embeddings=query_embedding, n_results=5)\")\nprint(\"# print(results)\")\nprint(\"```\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:00:15.125823Z","iopub.execute_input":"2025-04-23T11:00:15.126352Z","iopub.status.idle":"2025-04-23T11:01:21.643251Z","shell.execute_reply.started":"2025-04-23T11:00:15.126309Z","shell.execute_reply":"2025-04-23T11:01:21.642459Z"}},"outputs":[{"name":"stderr","text":"2025-04-23 11:00:28.258729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745406028.441747      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745406028.497923      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Kütüphaneler yüklendi.\nKullanılan Cihaz: cuda\n\nVeri klasörü kontrol ediliyor: /kaggle/input/gazete-txt\n5 adet .txt dosyası bulundu. Okunuyor...\n - Okundu: 20250410.txt\n - Okundu: 20250411.txt\n - Okundu: 20250407.txt\n - Okundu: 20250408.txt\n - Okundu: 20250409.txt\n\nToplam 5 doküman başarıyla okundu.\n\nMetinler parçalara (chunk) ayrılıyor...\nToplam 388 adet metin parçası (chunk) oluşturuldu.\n\nEmbedding modeli yükleniyor: BAAI/bge-m3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b9dac3cf784202bb837a5994c238e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9031bb241a494aa5b11d819fa76aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643d8da9edfd48c7bfd7c72c54b0a2dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56b466205a54dc88b92fe38774a4f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e726a40a57c348a9ad567f4d59e36448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d2cb3ecd85465491cf92345fd51c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6791d7b2c074902a113e1990308e810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1e439a278f4d32904d6d670253243a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529974c1bd6649b3904c4d33d7440472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dacccc4fb31c4fbe82342cba1599d4b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f76a8e89404d4e9f90cea3cc1e5e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5c49e053064f75894e34130b8eda79"}},"metadata":{}},{"name":"stdout","text":"Embedding modeli yüklendi.\n\nMetin parçaları için embeddingler oluşturuluyor (Bu işlem zaman alabilir)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1ecbf2174b4e41b576a6b33e382a2a"}},"metadata":{}},{"name":"stdout","text":"Embedding oluşturma tamamlandı. Süre: 15.04 saniye.\nOluşturulan embedding matris boyutu: (388, 1024)\n\nChromaDB kuruluyor ve veriler yükleniyor...\nVeritabanı yolu: /kaggle/working/chroma_db_bge_m3\n'resmi_gazete_bge_m3' koleksiyonu alındı veya oluşturuldu.\n388 adet chunk ChromaDB'ye ekleniyor...\nVeriler başarıyla ChromaDB'ye eklendi.\nKoleksiyondaki toplam öğe sayısı: 388\n\nİşlem tamamlandı!\nChromaDB vektör deposu '/kaggle/working/chroma_db_bge_m3' klasörüne kaydedildi.\n\nSonraki Adımlar:\n1. Bu Kaggle Notebook'unu 'Save Version' ile kaydedin (Type: 'Run All', Save Output: 'Yes').\n2. Kaydetme işlemi bittikten sonra, Notebook versiyon sayfasının 'Output' sekmesine gidin.\n3. Orada 'chroma_db_bge_m3' adlı klasörü bulun ve indirin.\n4. İndirdiğiniz bu klasörü lokal makinenizde RAG projenizde kullanabilirsiniz.\n\nLokalde Kullanım Örneği:\n```python\nimport chromadb\n# İndirdiğiniz klasörün yolunu verin\nLOCAL_CHROMA_PATH = '/path/to/your/downloaded/chroma_db_bge_m3'\nPERSISTENT_CLIENT = chromadb.PersistentClient(path=LOCAL_CHROMA_PATH)\nCOLLECTION = PERSISTENT_CLIENT.get_collection(name='resmi_gazete_bge_m3')\n# Sorgu yapmak için yine BAAI/bge-m3 modelini kullanmalısınız!\n# query_embedding = embedding_model.encode(['Sorgunuz buraya'])\n# results = COLLECTION.query(query_embeddings=query_embedding, n_results=5)\n# print(results)\n```\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- 7. ChromaDB Kaydını Doğrulama ve Zipleme ---\nprint(f\"\\nİşlem tamamlandı!\")\nprint(f\"ChromaDB vektör deposu '{CHROMA_PERSIST_DIR}' klasörüne kaydedildi.\")\n\n# Eklenen öğe sayısını tekrar kontrol edelim (isteğe bağlı ama iyi bir kontrol)\ntry:\n    final_count = collection.count()\n    print(f\"Koleksiyondaki öğe sayısı doğrulandı: {final_count}\")\n    if final_count == 0 and len(chunks) > 0: # Eğer chunk varsa ama koleksiyon boşsa uyarı ver\n         print(\"UYARI: Koleksiyonda hiç öğe bulunamadı. Ekleme sırasında bir sorun olmuş olabilir.\")\nexcept Exception as e:\n    print(f\"UYARI: Koleksiyon sayımı kontrol edilirken hata: {e}\")\n\n\n# --- 8. Output Klasörünü Zipleme ---\nimport zipfile\nimport shutil # shutil.make_archive daha kolay olabilir\n\nZIP_FILE_NAME = f\"{os.path.basename(CHROMA_PERSIST_DIR)}.zip\"\nZIP_FILE_PATH = os.path.join(\"/kaggle/working/\", ZIP_FILE_NAME) # Zip dosyasını /kaggle/working içine kaydet\n\nprint(f\"\\n'{CHROMA_PERSIST_DIR}' klasörü zipleniyor...\")\nprint(f\"Kaynak: {CHROMA_PERSIST_DIR}\")\nprint(f\"Hedef Zip: {ZIP_FILE_PATH}\")\n\ntry:\n    # shutil.make_archive kullanımı daha basit\n    shutil.make_archive(\n        base_name=os.path.join(\"/kaggle/working/\", os.path.basename(CHROMA_PERSIST_DIR)), # Zip dosyasının tam yolu (.zip olmadan)\n        format='zip',        # Arşiv formatı\n        root_dir=os.path.dirname(CHROMA_PERSIST_DIR), # Arşivlenecek klasörün üst dizini (/kaggle/working)\n        base_dir=os.path.basename(CHROMA_PERSIST_DIR) # Arşivlenecek klasörün adı (chroma_db_bge_m3)\n    )\n\n    # # Alternatif: zipfile modülü ile manuel zipleme (daha detaylı kontrol isterseniz)\n    # with zipfile.ZipFile(ZIP_FILE_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    #     # base_dir = os.path.dirname(CHROMA_PERSIST_DIR) # /kaggle/working\n    #     for root, dirs, files in os.walk(CHROMA_PERSIST_DIR):\n    #         for file in files:\n    #             file_path = os.path.join(root, file)\n    #             # Zip içindeki yolu ayarlamak için:\n    #             # /kaggle/working/chroma_db_bge_m3/file.txt -> chroma_db_bge_m3/file.txt\n    #             archive_name = os.path.relpath(file_path, os.path.dirname(CHROMA_PERSIST_DIR))\n    #             zipf.write(file_path, arcname=archive_name)\n\n    print(f\"'{ZIP_FILE_PATH}' başarıyla oluşturuldu.\")\n\n    # İsteğe bağlı: Zipleme sonrası orijinal klasörü silerek disk alanı açabilirsiniz\n    # Ancak output'ta hem klasör hem zip olsun isterseniz bu satırı yorumlayın/silin\n    # print(f\"Orijinal klasör siliniyor: {CHROMA_PERSIST_DIR}\")\n    # shutil.rmtree(CHROMA_PERSIST_DIR)\n\nexcept Exception as e:\n    print(f\"HATA: Zipleme sırasında bir hata oluştu: {e}\")\n\n\n# --- 9. Sonuç ve İndirme Talimatları ---\nprint(\"\\nSonraki Adımlar:\")\nprint(\"1. Bu Kaggle Notebook'unu 'Save Version' ile kaydedin (Type: 'Run All', Save Output: 'Yes').\")\nprint(\"2. Kaydetme işlemi bittikten sonra, Notebook versiyon sayfasının 'Output' sekmesine gidin.\")\nprint(f\"3. Orada '{ZIP_FILE_NAME}' adlı zip dosyasını bulun ve indirin.\")\nprint(\"4. İndirdiğiniz zip dosyasını lokal makinenizde uygun bir yere açın.\")\nprint(\"5. Açtığınız klasör ('chroma_db_bge_m3' adında olmalı), lokal RAG projenizde kullanacağınız vektör deposudur.\")\nprint(\"\\nLokalde Kullanım Örneği:\")\nprint(\"```python\")\nprint(\"import chromadb\")\nprint(\"# Açtığınız klasörün yolunu verin\")\nprint(f\"LOCAL_CHROMA_PATH = '/path/to/your/extracted/{os.path.basename(CHROMA_PERSIST_DIR)}'\") # Zip'ten çıkan klasörün yolu\nprint(f\"PERSISTENT_CLIENT = chromadb.PersistentClient(path=LOCAL_CHROMA_PATH)\")\nprint(f\"COLLECTION = PERSISTENT_CLIENT.get_collection(name='{CHROMA_COLLECTION_NAME}')\")\nprint(\"# Sorgu yapmak için yine BAAI/bge-m3 modelini kullanmalısınız!\")\nprint(\"# from sentence_transformers import SentenceTransformer\")\nprint(\"# embedding_model = SentenceTransformer('BAAI/bge-m3') # Modeli yükle\")\nprint(\"# query_embedding = embedding_model.encode(['Sorgunuz buraya']).tolist()\")\nprint(\"# results = COLLECTION.query(query_embeddings=query_embedding, n_results=5)\")\nprint(\"# print(results)\")\nprint(\"```\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:14:44.673170Z","iopub.execute_input":"2025-04-23T11:14:44.673798Z","iopub.status.idle":"2025-04-23T11:14:45.197223Z","shell.execute_reply.started":"2025-04-23T11:14:44.673775Z","shell.execute_reply":"2025-04-23T11:14:45.196433Z"}},"outputs":[{"name":"stdout","text":"\nİşlem tamamlandı!\nChromaDB vektör deposu '/kaggle/working/chroma_db_bge_m3' klasörüne kaydedildi.\nKoleksiyondaki öğe sayısı doğrulandı: 388\n\n'/kaggle/working/chroma_db_bge_m3' klasörü zipleniyor...\nKaynak: /kaggle/working/chroma_db_bge_m3\nHedef Zip: /kaggle/working/chroma_db_bge_m3.zip\n'/kaggle/working/chroma_db_bge_m3.zip' başarıyla oluşturuldu.\n\nSonraki Adımlar:\n1. Bu Kaggle Notebook'unu 'Save Version' ile kaydedin (Type: 'Run All', Save Output: 'Yes').\n2. Kaydetme işlemi bittikten sonra, Notebook versiyon sayfasının 'Output' sekmesine gidin.\n3. Orada 'chroma_db_bge_m3.zip' adlı zip dosyasını bulun ve indirin.\n4. İndirdiğiniz zip dosyasını lokal makinenizde uygun bir yere açın.\n5. Açtığınız klasör ('chroma_db_bge_m3' adında olmalı), lokal RAG projenizde kullanacağınız vektör deposudur.\n\nLokalde Kullanım Örneği:\n```python\nimport chromadb\n# Açtığınız klasörün yolunu verin\nLOCAL_CHROMA_PATH = '/path/to/your/extracted/chroma_db_bge_m3'\nPERSISTENT_CLIENT = chromadb.PersistentClient(path=LOCAL_CHROMA_PATH)\nCOLLECTION = PERSISTENT_CLIENT.get_collection(name='resmi_gazete_bge_m3')\n# Sorgu yapmak için yine BAAI/bge-m3 modelini kullanmalısınız!\n# from sentence_transformers import SentenceTransformer\n# embedding_model = SentenceTransformer('BAAI/bge-m3') # Modeli yükle\n# query_embedding = embedding_model.encode(['Sorgunuz buraya']).tolist()\n# results = COLLECTION.query(query_embeddings=query_embedding, n_results=5)\n# print(results)\n```\n","output_type":"stream"}],"execution_count":3}]}